---
title: 大模型研究笔记
math: true
excerpt: 这是摘要
date: 2024-02-29 23:47:11
tags: 
categories: 开发笔记
---
最近在研究大模型，在此记录一下一些信息。

## 理论
### 模型训练epoch越多效果越差？
1.大模型训练时，epoch越多效果越差？ https://mp.weixin.qq.com/s/DBP_eafGeKMEuSIma9Z9Tg

### instruct、base、chat模型有什么区别
OpenAI的模型主要分为三类：`instruct`、`base`和

chat

。它们的主要区别如下：

1. **Base 模型**：
   - 这是最基础的模型，没有经过任何特定任务的微调。
   - 适用于广泛的自然语言处理任务，但需要用户提供明确的指令和上下文。

2. **Instruct 模型**：
   - 这些模型经过微调，专门用于遵循指令。
   - 用户可以直接给出指令，模型会根据指令生成相应的输出。
   - 例如，`text-davinci-002`和`text-davinci-003`是典型的Instruct模型。

3. **Chat 模型**：
   - 这些模型经过微调，专门用于对话场景。
   - 它们能够更好地理解和生成对话内容，适用于聊天机器人和对话系统。
   - 例如，`gpt-3.5-turbo`和`gpt-4`是典型的Chat模型。

总结：
- **Base 模型**：通用模型，需要明确指令和上下文。
- **Instruct 模型**：专门用于遵循指令，适合任务导向的应用。
- **Chat 模型**：专门用于对话场景，适合聊天机器人和对话系统。

根据具体需求选择合适的模型类型，可以更好地满足应用场景的要求。


## 量化
[[llama.cpp] 新量化方法速度效果测试（持续更新，6月19日） · ymcui/Chinese-LLaMA-Alpaca · Discussion #513 (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/513)
[GGUF量化方法概述：r/LocalLLaMA --- Overview of GGUF quantization methods : r/LocalLLaMA (reddit.com)](https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/)
[AI大模型：一文搞懂大模型文件存储格式新宠GGUF-CSDN博客](https://blog.csdn.net/pythonhy/article/details/142919327)

## Qwen Agent

### llama server部署
这里暂时先直接使用sakura llm的server来提供api，关于llama server部署的细节有待以后探索。

```bat
%common.bat

@echo off
@chcp 65001 > nul %设置代码页为UTF-8

setlocal enableextensions enabledelayedexpansion %启用命令扩展和延迟变量扩展

if /i "%label%"=="" (
	echo 不可以直接调用这个脚本
	goto quit
)

@title 启动Sakura服务器-%label%

set n=0
for /f "delims=" %%i in ('where .:*.gguf') do (   %扫描文件夹下的gguf模型文件
	set models[!n!].path=%%i
	set models[!n!].name=%%~ni
	set /a n+=1
)

if %n% equ 0 goto no-model
if %n% equ 1 goto one-model
goto many-model

:no-model
echo 没有检测到gguf模型文件,请确定将模型文件放到了当前文件夹
goto quit

:one-model
set model.name=%models[0].name%
set model.path=%models[0].path%
goto launch

:many-model
set /a end=%n%-1

echo 请输入数字来选择要使用的模型,默认选择0
for /l %%i in (0,1,%end%) do (
	echo %%i. !models[%%i].name!
)
echo.
:choice-model
set choice=
set /p choice= 请选择:
if /i "%choice%"=="" set choice=0
for /l %%i in (0,1,%end%) do (
	if /i "%choice%"=="%%i" (
		set model.name=!models[%%i].name!
		set model.path=!models[%%i].path!
		goto launch
	)
)
echo 选择⽆效,请重新输⼊
goto choice-model

:launch
@title 启动Sakura服务器-%label%-%model.name%
echo.
echo 模型名称：%model.name%
echo 模型路径：%model.path%
echo.
echo 准备启动Sakura服务器...
@echo on
.\llama\server.exe -m .\%model.name%.gguf -c 2048 -ngl %ngl% -a %model.name% --host 127.0.0.1 --port 8080
% -c 提示上下文大小 -ngl VRAM(显存）中存储的层数 -a 为模型名称设置别名

@echo off

goto quit

:quit
echo.
echo 按任意键退出
pause > nul
exit
```

```bat
%启动.bat
@echo off
@chcp 65001 > nul

set label=显卡
set ngl=999 %把所有层都放到显存里
call common.bat
```

然后去huggingface或者魔搭社区下载qwen 2.5 的**GGUF**格式的模型文件（权重），扔到llama cpp文件夹下启动就可以了。默认api地址为127.0.0.1:8080
### 调用API
[llama.cpp/examples/server/README.md](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)
由于openai库的某次更新，中文互联网+github copilot生成的代码都不能直接用，因此最好还是从llama.cpp的官方文档里找教程。
参见“ POST `/v1/chat/completions`: OpenAI-compatible Chat Completions API ” 一节

``` python

import openai

client = openai.OpenAI(
    base_url="http://localhost:8080/v1", # "http://<Your api-server IP>:port"
    api_key = "sk-no-key-required"
)

completion = client.chat.completions.create(
model="gpt-3.5-turbo", #离谱的地方在于，模型名称不对完全不影响输出
messages=[
    {"role": "system", "content": "You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests."},
    {"role": "user", "content": "你是谁"}
]
)

print(completion.choices[0].message)

```
然后得到了一个诡异的输出：
```Qwen
ChatCompletionMessage(content='
我是! 一个!!!!! 人工智能!!!!! 助!手!!!!! 我!!!!! 叫!!!!! ChatGPT!!!!! 我!!!!!! 来!!!!! 帮!助!!!!!!! !!你!!!!!! 
了!!!!! 解!决!!!!!!!!!!! !!!!问题!!和!!提!供!!!帮助!!!!!! ！如果你!!!有!!!任何!!问题!!或!需要!!帮助!，!!请!随时!!告诉我!！
', role='assistant', function_call=None, tool_calls=None)

```

Question：为什么这个输出如此诡异？
Answer：