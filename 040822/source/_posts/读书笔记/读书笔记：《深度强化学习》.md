---
title: 读书笔记：《深度强化学习》
math: true
excerpt: 这是摘要
date: 2024-02-20 23:47:38
tags: 读书笔记
categories: 读书笔记
sticky: 
---
公式渲染有问题，不知道为什么。

# 1 机器学习基础  
## 1.1 线性模型  
### 1.1.1 线性回归
线性回归：$f(x;w,b)=x^T * w +b$  
其中x:特征（输入）向量；w：权重 ；b偏移量（offset）  
  
最小二乘法：  
损失函数：$L(w,b) = \frac{1}{2n} \sum \limits_{i=1}^n{[f(x_i;w,b) - y_i]^2}$   
优化目标： $\mathop{\min}\limits_{w,b}L(w,b)$  
为防止训练数据过少而造成的过拟合，常使用正则化：$\mathop{\min}\limits_{w,b}L(w,b) + \lambda R(w)$  
其中R(w)是正则项，$\lambda$是超参数。

### 1.1.2 逻辑斯谛回归
回归问题：预测一个连续值y=f(x)  

二分类问题：y=0或1  

$sigmoid(z) :=\frac{1}{1+exp(-z)}$  

交叉熵：设p、q向量表示两个m维离散概率分布，概率非负，和为1，则：$H(p,q)=- \sum \limits_{i=1}^m{p_i * ln (q_j)}$  

熵：$H(p)=H(p,p)$  

KL散度：用于衡量两个概率分布的区别有多大   
$$KL(p,q)= H(p,q)-H(p) $$ 
$$
\qquad\quad=\sum \limits_{i=1}^m{p_i * ln (\frac{p_i}{q_j})}$$  
显然，当q确定时，关于p的优化KL散度等于优化交叉熵。

### 1.1.3 softmax分类器
多分类问题：y=0、1、2……等多个类别。  

one-hot编码：把每个类表示成一组基向量，比如现在有2个类，则:   
0=>[1,0]  
1=>[0,1]

$softmax(z) :=\frac{1}{\sum exp(z_i)} *[exp(z_1),exp(z_2),...,exp(z_k)]^T$  
相当于把输入向量的每个元素先exp再归一化，输出向量各项非负且和为1. 

## 1.2 神经网络  
### 1.2.1 全连接神经网络  
全连接层包括线性函数和激活函数,比如
$y=sigmoid(w*x+b)$

多层感知器（MLP）：n个全连接层合在一起。

### 1.2.2 卷积神经网络
没细讲。
### 1.3 梯度下降和反向传播

## 小结
线性回归=》回归问题=》线性函数  
逻辑斯谛回归=》单分类问题=》线性函数+sigmoid  
softmax分类器=》多分类问题=》线性函数+softmax  

习题：
1.一般需要对参数过多的全连接层做正则化。

# 2 蒙特卡洛方法  
## 2.1 随机变量  
讲概率论与数理统计的基础知识，包括：
随机变量、观测值、累积分布函数、概率质量函数/概率密度函数、期望、随机抽样
## 2.2蒙特卡洛方法实例
蒙特卡洛：设x为观察值，构建f(x)，样本足够多的时候，f(x)->E[f(x)]。
