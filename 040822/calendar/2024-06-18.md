
## 环境变量
![[IMG-20240618165900357.png]]
[解决windows下环境变量太大的问题（终极！！可用）_环境变量 缩起来-CSDN博客](https://blog.csdn.net/github_34777264/article/details/85342877)
注意：环境变量是exe所在目录，不是exe的地址
新增变量为%Path2%
[Windows添加环境变量是否需要重启-CSDN博客](https://blog.csdn.net/ayroun8793/article/details/101791158)
## ffmpeg
[FFmpeg教程（超级详细版） - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000042391748)

## llm
[(99+ 封私信 / 82 条消息) 2024年大模型LLM还有哪些可研究的方向？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/637595961/answer/3484098345)
### scaling law
[解析大模型中的Scaling Law - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/667489780)
- 对于decoder-only的模型，计算量C flops，模型参数量N，数据大小D tokens，满足   $C \approx 6ND$
- 模型的性能主要与CND三者相关，与模型的结构参数（层数、深度、宽度）基本无关
- 当不受CND三者中的其他两个制约时，模型性能与每个因素都呈现幂律关系，例如: $性能 \sim ln(N)$
![[IMG-20240618214100580.png]]


### 显存计算
[[LLM]大模型显存计算公式与优化 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/687226668)
[大模型运行推理显存计算器 - 任霏的个人博客网站 (renfei.net)](https://www.renfei.net/kitbox/model-memory-calculator)

推理的显存占用理论值计算公式：

| 量化精度 | 占用字节 |
| ---- | ---- |
| fp32 | 4    |
| bf16 | 2    |
| fp16 | 2    |
| int8 | 1    |
| int4 | 0.5  |



### 量化
[[llama.cpp] 新量化方法速度效果测试（持续更新，6月19日） · ymcui/Chinese-LLaMA-Alpaca · Discussion #513 (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions/513)
[GGUF量化方法概述：r/LocalLLaMA --- Overview of GGUF quantization methods : r/LocalLLaMA (reddit.com)](https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/)


### 模型占用实验
感谢kaggle，能让我们白嫖15Gx2显存和高达近1000Mbps的下载带宽，因此这里将以sakuraLLM为例，测试各种模型的实际显存占用情况。
吐个槽，在测试32b iq4xs的时候，kaggle硬盘爆了，这是因为kaggle下载llama.cpp和模型的时候会把模型放在output文件夹里边，然后output文件夹限容为19.2G，还没有显存和内存大QAQ。


| 模型                                     | 参数量 | 量化等级  | 模型大小（占用硬盘空间） | 占用显存大小 | 显存占用与硬盘占用比 | 评价                                |
| -------------------------------------- | --- | ----- | ------------ | ------ | ---------- | --------------------------------- |
| sakura-32b-qwen2beta-v0.9.1-iq4xs.gguf | 32B | iq4xs | 17.7G        | 18.2G？ | 1.03       | 占用比居然这么低                          |
| sakura-14b-qwen2beta-v0.9.2-iq4xs.gguf | 14B | iq4xs | 7.91G        | 11.2G  | 1.42       | 适合12G显卡                           |
| sakura-14b-qwen2beta-v0.9.2-q3km.gguf  | 14B | q3km  | 7.42G(好大)    | 10.8G  | 1.46       | 与iq4xs相比没有明显优势                    |
| sakura-14b-qwen2beta-v0.9.2-q2k.gguf   | 14B | q2k   | 2.37G        | 加载不了   | 模型有问题      | 同左                                |
| GalTransl-7B-v1-Q6_K.gguf              | 7B  | q6k   | 6.34G        | 8.2G   | 1.29       | 比较尴尬的显存占用                         |
| GalTransl-7B-v1-IQ4_XS.gguf            | 7B  | iq4xs | 4.29G        | 6.6G   |            | 本地电脑的显存占用只有5.5G，奇怪。会不会是因为双卡部署的问题？ |
|                                        |     |       |              |        |            |                                   |
|                                        |     |       |              |        |            |                                   |
