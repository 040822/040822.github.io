---
title: 读书笔记：《深度强化学习》
math: true
excerpt: 这是摘要
date: 2024-02-20 23:47:38
tags: 读书笔记
categories: 读书笔记
sticky: 
---
公式渲染有问题，不知道为什么。

# 1 机器学习基础  
## 1.1 线性模型  
### 1.1.1 线性回归
线性回归：$f(x;w,b)=x^T * w +b$  
其中x:特征（输入）向量；w：权重 ；b偏移量（offset）  
  
最小二乘法：  
损失函数：$L(w,b) = \frac{1}{2n} \sum \limits_{i=1}^n{[f(x_i;w,b) - y_i]^2}$    （mse） 
- [ ] 为什么是1/2n？ 虽然常数系数影响不大。
优化目标： $\mathop{\min}\limits_{w,b}L(w,b)$  
为防止训练数据过少而造成的过拟合，常使用正则化：$\mathop{\min}\limits_{w,b}L(w,b) + \lambda R(w)$  
其中R(w)是正则项，可以为w的某个范数的函数，$\lambda$是超参数。

### 1.1.2 逻辑斯谛回归
回归问题：预测一个连续值y=f(x)  

二分类问题：y=0或1  

$sigmoid(z) :=\frac{1}{1+exp(-z)}$   激活函数

交叉熵：设p、q向量表示两个m维离散概率分布，概率非负，和为1，则：$H(p,q)=- \sum \limits_{i=1}^m{p_i * ln (q_j)}$  

熵：$H(p)=H(p,p)$  

KL散度：用于衡量两个概率分布的区别有多大   
其中p、q为两个概率分布
$$KL(p,q)= H(p,q)-H(p) $$ 
$$
\qquad\quad=\sum \limits_{i=1}^m{p_i * ln (\frac{p_i}{q_j})}$$  
显然，当q确定时，关于p的优化KL散度等于优化？交叉熵。

### 1.1.3 softmax分类器
多分类问题：y=0、1、2……等多个类别。  

one-hot编码：把每个类表示成一组基向量，比如现在有2个类，则:   
0=>[1,0]  
1=>[0,1]

$softmax(z) :=\frac{1}{\sum exp(z_i)} *[exp(z_1),exp(z_2),...,exp(z_k)]^T$  
相当于把输入向量的每个元素先exp再归一化，输出向量各项非负且和为1. 

## 1.2 神经网络  
### 1.2.1 全连接神经网络  
全连接层包括线性函数和激活函数,比如
$y=sigmoid(w*x+b)$

多层感知器（MLP）：n个全连接层合在一起。

### 1.2.2 卷积神经网络
没细讲。
### 1.3 梯度下降和反向传播
梯度: 
$$\nabla_{w_1} L = \frac{\partial L(w_1,w_2,...,w_i,...)}{\partial w_i}$$

结论：梯度的形状与w形状一致。

梯度下降GD：更新参数 $w_{new} = w_{old} - a * \nabla_{w_1} L$
a代表学习率。

随机梯度下降SGD：每次更新时只选取部分样本计算L。
$$L(w)=\frac{1}{n} \Sigma F_j(w)$$
GD问题会陷在鞍点，收敛不到局部最优，因此实际通常会用SGD及其变种。

反向传播BP：使用链式法则计算参数梯度。

## 小结
线性回归=》回归问题=》线性函数  
逻辑斯谛回归=》单分类问题=》线性函数+sigmoid  
softmax分类器=》多分类问题=》线性函数+softmax  

习题：
1.一般需要对参数过多的全连接层做正则化。

# 2 蒙特卡洛方法  
## 2.1 随机变量  
讲概率论与数理统计的基础知识，包括：
随机变量、观测值、累积分布函数、概率质量函数/概率密度函数、期望、随机抽样
## 2.2蒙特卡洛方法实例
蒙特卡洛MC：设x为观察值，构建f(x)，样本足够多的时候，f(x)->E[f(x)]。或者说，抽样，使频率逼近概率。
收敛速度$O(\frac{1}{\sqrt{n}})$
优化方法：拟蒙特卡罗QMC，收敛速度$O(\frac{1}{n})$
示例：1.计算pi 2. 估算面积 3.计算定积分（问：MC和其他数据计算方法的优劣） 4.近似期望 5.随机梯度-用MC近似梯度


# 3 强化学习基本概念

## 3.1 马尔可夫决策过程
MDP：状态 动作 奖励
状态转移函数:  $p_t(s'|s,a) = P(S_{t+1}^{'}=s'|S_t=s,A_t=a)$

## 3.2 策略
给定某个状态，给出做各种动作的概念。比如看到对方残血，一种策略为补刀的概率为0.9，逃跑的概率为0.1
策略函数： $\pi(a|s) = P(A=a|S=s)$
确定性策略：看到某个状态，就做某个动作，或者说这个动作的概率为1

## 3.3 随机性
马尔可夫性质： $P(S_{t+1}|S_t,A_t) = P (S_{t+1}|S_1,A_1,S_2,A_2,...)$
轨迹：$S_1,A_1,S_2,A_2,...$

动作的随机性： 来自策略
奖励的随机性： 来自$A_t$或$(S_t,A_t)$

## 3.4 回报
回报 return： 从当前时刻及未来时刻的奖励之和。（为什么要算入未来的奖励？因为当前时刻的动作对未来产生了影响）
强化学习的优化目标：使return最小

回报
1.普通回报： $U_t=\Sigma_{i=t}^{n} R_i$
2.折扣回报： $U_t=\Sigma_{i=t}^{n} R_i*\gamma^{i-t}$
折扣回报的直观理解：现在时刻的动作对现在时刻的影响更大，对未来的影响更小，因此给一个衰减因数。

有限期无限期MDP：折扣率为1时，无限期MDP的回报可能不收敛。

回报中的随机性：来自A（t到n），S（t+1到n）

## 3.5 价值函数

### 3.5.1 动作价值函数 Q
