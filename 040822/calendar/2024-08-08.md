[(38 条消息) CS类AI大一进组科研，如何入门学习机器学习及其相关基础? - 知乎 (zhihu.com)](https://www.zhihu.com/question/569151613/answer/3008238260)


## 为什么所有公开的 GPT-3 复制都失败了？我们应该在哪些任务中使用 GPT-3.5/ChatGPT？
[为什么所有公开的 GPT-3 复制都失败了？我们应该在哪些任务中使用 GPT-3.5/ChatGPT？ --- Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT? (jingfengyang.github.io)](https://jingfengyang.github.io/gpt)
一篇有点年代的文章，写于2023 年 2 月 12 日，当时可以说市面上完全没有能与GPT3.5匹敌的llm，不管是知名度还是能力，GPT3.5都是the only one，直到2023年3月14日OpenAI发布gpt4，才宣告gpt3.5时代的终结。至于当时其他大模型？从本文所列举的gpt3.5的对手就可以看出来了：PaLM、BLOOM、OPT、FLAN-T5/PaLM、HELM、GLM，其中除了GLM活到了今天，更迭了四代，别的都已经成为llm发展的基石了。

### 为什么所有公开复制的 GPT-3 都失败了？
这个标题显然已经成为老黄历了，我们不妨给它换个标题：为什么GPT-3成功了？
llm斗蛐蛐提示：作者认为GPT-3和PaLM算成功，BLOOM、OPT算比较失败的

### 预训练数据
- 高质量数据。数据量较少的高质量数据集的效果大于数据量较多的质量不高的数据集，这已经成为业界共识了。但同时，如果数据量相差过大的话，那么数据量更大的数据集的效果肯定是更好的，正所谓力大砖飞。数据多样性和质量之间的权衡是个问题，不知道有没有什么更深入的研究。TODO
- 预训练数据去重。去重有利于避免记忆和过拟合（记忆：大模型倾向于直接输出看过的数据，而不是换种说法。比如大模型看过的数据：“去重有利于避免记忆和过拟合” 记忆输出：“去重有利于避免记忆和过拟合”（一模一样） 泛化输出：“避免过拟合的一种方法是去重”） 但必须指出，记忆和过拟合稍有区别，同时记忆在特定情况下并不代表性能一定差，下边还会提。 作者也指出，去重对于避免过拟合的重要性有待商榷。（实际上大模型“过拟合”现象本身就有待商榷。要知道大模型通常仅训练一轮，并且数据集通常杂七杂八五花八门，即使有少量文本重复，从整体来看重复率也不会很高，为什么会出现过拟合现象？并且，大模型的过拟合现象指的是什么，是记忆吗？这一部分也有待我们去看看有没有研究）TODO
- 预训练数据的多样性。包括领域多样性、形式多样性（例如文本、代码、表格）和语言多样性。作者认为BLOOM性能差的一个原因是因为训练数据中学术资料占比大，而日常语料占比少。而chatGPT流行开来的一个原因可能就是日常语料占比较大。

注意：上边三点适用于通用模型，垂直模型的情况会不一样。同时，用户端是感受不到模型的记忆现象的，对于chat agent，用户通常更关心回答的准确性。比如翻译llm相关文章的时候，用户显然更喜欢一个（LLM=》大语言模型  98%， LLM=》法学硕士 2%）的模型，而不是（LLM=》大语言模型  80%， LLM=》法学硕士 20%）的模型，尽管前者更可能有一定的记忆现象。对于垂直模型而言，或许适度的记忆并不是坏事。

作者的注意：同时，特定预训练数据对llm在特定下游任务上的性能影响很大。比如多语言数据会使llm更擅长多语言任务和机器翻译。这对于垂直模型具有指导性意义。
```
一个有趣的现象是，尽管在预训练过程中使用了代码数据，但 BLOOM 在编程和 CoT 能力上仍然表现不佳，这可能表明仅使用代码数据无法保证模型的编程和 CoT 能力。
```

### 训练策略
感觉一般人没必要看这一段，毕竟能有资格从头训练大模型的人真的不多。
在这里，训练策略包括训练框架、训练周期长度、模型架构/训练设置、训练过程中的修改，目的是在训练非常大的模型时获得更好的稳定性和收敛性。
损失突变和发散现象被广泛观察到，原因不明。

#### 训练框架 
数据并行模型并行没必要看，接触不到。
有个有趣的地方：llm训练中，bf16比fp16好（因为llm的权重对于小数点后过多位数不敏感，且bf16可以处理训练损失峰值时的大数值），但V100混合精度只能用fp16，因此这可能导致了OPT的不稳定。 总结：别用V100，用A100

#### 训练过程中的修改
OPT做了一堆修改，可能反而造成了它的失败；PaLM几乎没有修改，反而成功了。
这段本身没啥意义，我们可以关注一下OPT文章中对其的一些修改的讨论（比如SGD换Adam）

#### 模型架构/训练设置
也有点太细节了，对读者而言意义不大。
作者提到，GLM-130B使用基于 DeepNorm 的 Post-LN 而不是 Pre-LN，知乎上某巨佬对此做了一个非常精彩的理论解释。[[Transformer 101系列] 初探LLM基座模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/640784855)
Encoder-only=》LayerNorm部分

#### Training duration
![[Pasted image 20240809010236.png]]
因此，使用更多令牌和更大质量的语料库进行预训练可能是 GPT3 和 PaLM 成功的重要因素。
比较有意思的点在于：PaLM和GPT3看的比总共的少，OPT和BLOOM反过来，这说明后者出现了数据复用（重复），其中OPT很可能训练了两轮。这或许是这俩失败的一个原因。
还有一个点在于：GPT3和OPT都看了300B，但是数据库大小却不一样，这或许说明，保持看到的总数不变下，更大的数据库也会使性能提高？

#### 其他
一堆细节。

首先，PaLM 和 GPT-3 在训练过程中使用了逐渐增加（从小到大）的批大小，这已被证明对训练更好的LLM非常有效，而 OPT 和 BLOOM 使用了恒定的批大小。
这里有点迷惑，为什么逐渐增加的batch size会有更好的效果？一般认为大batch size会有好效果，为什么不能一上来就把batch size加到最大呢？ TODO

其次，OPT 使用了 ReLU 激活，而 PaLM 使用了 SwiGLU 激活，GPT-3/BLOOM 使用了 GeLU 激活，这通常会导致训练LLM更好的性能。

最后，为了更好地建模更长的序列，PaLM 使用了 RoPE 嵌入，而 BLOOM 使用了 ALiBi 嵌入，而原始的 GPT-3 和 OPT 使用了学习位置嵌入，这可能会影响长序列的性能。

这俩是两个细节问题，目前涉及不到，日后讨论。

### 在哪些任务中应该使用 GPT-3.5/ChatGPT？
如今我们或许更应该问：哪些任务用不到GPT3.5？

作者的出发点在于，GPT3.5和较小的模型之间的抉择，在API价格日益下降的今天，对于大多数公司而言API的性价比要更高，同时今天已经有更多的模型供我们选择，因此本章的价值已经不大了。
这一章提到很多GPT-3.5/ChatGPT的优势任务，可供参考。

### 总结
前两部分分别从数据和模型/训练的角度进行切入，我们可以惊奇地发现，在llm领域中有关数据的讨论的重要性日益增长，而对于llm时代之前的深度学习领域，人们更多地是重视模型结构而非数据。
这或许是因为Transformer、GPT的成功和其他结构的失败使得后来者产生了路径依赖，同时也有可能是由于大模型训练成本过高，业界对于探索新模型结构的积极性并不大。

有关此处的讨论请见：[[2303.10158] 数据为中心的人工智能：一项综述 --- [2303.10158] Data-centric Artificial Intelligence: A Survey (arxiv.org)](https://arxiv.org/abs/2303.10158)
[daochenzha/data-centric-AI: A curated, but incomplete, list of data-centric AI resources. (github.com)](https://github.com/daochenzha/data-centric-AI)

TODO：
- 数据多样性和质量之间的权衡
- llm的过拟合问题
- batch size问题（由小变大）




## Kaggle：从入门到跑路
### 起点：泰坦尼克号
[泰坦尼克号 - 灾难中的机器学习 | Kaggle --- Titanic - Machine Learning from Disaster | Kaggle](https://www.kaggle.com/competitions/titanic)
[泰坦尼克号教程 --- Titanic Tutorial (kaggle.com)](https://www.kaggle.com/code/alexisbcook/titanic-tutorial)
#### 题目：
输入：乘客信息，如姓名、年龄、性别、社会经济阶层（字符串（姓名和性别）+序号+数值）
输出：乘客是否死亡（1没死0死）
`Train.csv` 将包含机上乘客子集（n=891）的详细信息，包含是否死亡
test.csv（n=418）

分析：一个二分类问题。

#### 教程

