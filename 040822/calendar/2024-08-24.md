
## SNN
今天刷文章的时候看到了个新概念，脉冲神经网络SNN：
[如何看待第三代神经网络SNN？详解脉冲神经网络的架构原理、数据集和训练方法 原创-CSDN博客](https://blog.csdn.net/Extremevision/article/details/123853471)
[简述脉冲神经网络SNN：下一代神经网络 | 机器之心 (jiqizhixin.com)](https://www.jiqizhixin.com/articles/2018-01-13-7)
仿真学上：依据神经元膜电位进行建模，使SNN神经元更仿真
数学建模上：变成了微分方程（不过实际应用中似乎是使用离散时间来做的，这就转化成为差分方程了）。
输入输出上：输入使膜电位上升，只有膜电位达到阈值之后，才有输出（输出电位 置为一，这里输出为二元的）
那么SNN的特点在于什么？内置时间衰退，阈值相当于一个激活函数，只有有足够脉冲输出时，下一个神经元才有可能被激活，也就是说激活的神经元是很稀疏的——稀疏性可能会造成神经元的浪费。
问题很显然：微分方程增加计算难度；输出二元化，精度感觉会下降，稀疏性也会带来很多。机器之心的那篇文章认为SNN比mlp更好，但是我并没有感觉到。此外，感觉通过简单的改造mlp似乎也能实现相应的效果？把激活函数设置为二元输出（起个名字，01-relu），更新参数时固定减个值实现衰退。
此外，如果要从“仿真”的意义上来实现对人类大脑的仿真的话，那么SNN应该建立起一个极其复杂的神经网络结构，而不是mlp那种一层一层的、层次化的“简单”网络结构，这对于训练来说是个挑战。但考虑到输入和输出层的存在，似乎SNN也只能做成层次化的结构？如果想要加强仿真程度的话，就要不预设“输入”和“输出”突触的概念，而是所有突触都能起到输入和输出的作用：这会大大提升神经网络的复杂性，究竟是好是坏咱也不知道。
目前来看SNN并没有流行起来，有时间可以搜一下相关论文。

