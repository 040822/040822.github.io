## 
[(43 条消息) 实在欣赏不了《大堰河，我的保姆》怎么办？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/265666052)
[与乳母们竞争，为不哺乳撑腰：奶粉的形象工程是如何一步步建立起来的？|界面新闻 · 文化 (jiemian.com)](https://www.jiemian.com/article/4958562.html)

[有想入坑RL-LLM的同学吗？强推曾经的TimeChamber，一个GPU够了 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/715131589)

## The Llama 3 Herd of Models
很长的一篇技术报告，不知道为什么zotero的pdf渲染有问题，学术gpt的tex翻译好了，但是tex2pdf也有问题，只能看原文了。
甚至edge的pdf阅读器都存在渲染问题，离谱，第一次见到这种情况。

### Abstract
发布了一群llama3模型，有多语言能力，多模态能力。

### 1 Introduction
基础模型训练包括：1.预训练，在一些基础任务上进行训练，比如预测下一个单词等等。2.post-training，微调模型以让模型能够跟随指令以及拥有一些具体能力（比如coding）

llm的三个关键方法：data, scale, and  managing complexity
- data：预训练15T，而llama2只用了1.8T
- scale：3.8e25 Flops，比llama2多50倍
- managing complexity：使用普通的transformer而不是MoE，以最大化训练时的稳定性；微调时也只用SFT，RS，DPO而不是更复杂的强化学习算法。

下边就是介绍模型能力、打榜什么的。

### 2 General Overview
先预训练c3后微调c4~c5，又搞了3个额外阶段：
- Multi-modal encoder pre-training：train separate encoders for images and speech.