---
title: 大模型折腾实验报告1
math: true
excerpt: 这是摘要
date: 2024-03-01 23:56:55
tags:
categories:
sticky:
---
最近根据[^1][^2][^3]折腾了一个简单的人工智障大模型，在这里分析并记录一下这次折腾的经历。  
TODO：引用功能
[^1]:https://github.com/DLLXW/baby-llama2-chinese
[^2]:https://zhuanlan.zhihu.com/p/652664029
[^3]:https://zhuanlan.zhihu.com/p/660759033
# 0.开发环境
操作系统：windows 11  
显卡：3060 6G  

# 1.实验流程简要版
```
git clone          
pip install -r requirements.txt
手动下载预训练用和微调用数据
python data_process.py #预处理数据
python pretrain.py #预训练，需要调参
python sft_data_process.py #预处理微调数据
python sft.py #微调
python eval.py #评估
```

# 2.代码分析

## 2.1 model.py

大佬手敲了一个transformer，正好便于我们学习transformer的实现代码  

### RMSNorm

[比较 Layer Norm、RMS Norm、Deep Norm](https://zhuanlan.zhihu.com/p/620297938)
LayerNorm：常用归一化方式，用于稳定分布[text](https://www.zhihu.com/question/487766088)[text](https://zhuanlan.zhihu.com/p/492803886)。
RMSNorm：LN的简化版本，可以减少计算量
DeepNorm：用于Post-LN，参见[text](https://zhuanlan.zhihu.com/p/640784855)

## Attention

- QKV：输入向量x经过线性层后输出Q、K、V向量
- RoPE：旋转式位置编码，见下。
- 以下是manual implementation
- QK点积：score = Q*K/sqrt(dim)    #为什么除以sqrt(dim)参见[text](https://zhuanlan.zhihu.com/p/640784855)
- mask掩码：score = score + mask
- 过softmax: score = softmax(score)
- dropout
- SV点积： output=score*V
- output处理： output过线性层+dropout
- 另一种实现是flash implementation，使用torch.nn.functional.scaled_dot_product_attention
- 缩放点积注意力


## FeedForward

原版transformer的FFN好像就是低维=》高维的线性层+ReLU+高维=》低维线性层+ReLU，这里可能是变种。  

- 有w1 w2 w3三个线形层
- x过w1再过silu（Sigmoid Linear Unit）得到o1
- x过w3得到o3
- o1*o3之后过w2，结果dropout后输出

其中w1、w2、w3的维度很有意思。

## TransformerBlock

- 注意+残差： h = attention(RMSNorm(x)) + x
- FFN+残差： output = ffn(RMSNorm(h)) + h

属于Pre-LN的方式。

## Transformer
重头戏来了！  
Transformer所做的最重要的是就是把n_layers个Transformer Block连在一起。对此，我们可以研究一下各个大模型是如何连接Transformer Block的。  

### Bert
![bert](https://github.com/040822/picx-images-hosting/raw/master/v2-e162330dd6ecec3422548465fdb50cb4_720w.8hg7t5t5m0.webp)  
由上图可见，Bert使用的是Post-Norm的结构。

-2024.03.02更新至此
