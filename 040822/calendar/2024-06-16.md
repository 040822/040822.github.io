## [[2309.08632] Pretraining on the Test Set Is All You Need (arxiv.org)](https://arxiv.org/abs/2309.08632)
幽默，有趣的一项研究。作者使用“精心准备”的非合成的100k tokens语料库训练了一个1m（或者说0.001b）大小的“大”模型，phi-CTNL（读作fictional，虚构的），实现了前所未有的训练效率和grokking能力：
![[Pasted image 20240616122111.png]]
phi-CTNL在基准测试中成功击败了那些比它大几千~几百万倍、数据量比它大n倍、训练时间比它长n倍的所有已知模型，这充分的说明了构建高质量数据集以高效训练模型的重要性。
[[2309.08632] Pretraining on the Test Set Is All You Need (arxiv.org)](https://arxiv.org/abs/2309.08632)


## sakuraLLM+Luna Translator使用记录

[[sakuraLLM+Luna Translator使用记录]]
