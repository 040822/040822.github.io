[alphaXiv](https://alphaxiv.org/)
斯坦福搞的一个arxiv社区，能看arxiv文章并发表评论。

以下为这周的推荐论文，我挑了一些感兴趣的。
##  KAN or MLP: A Fairer Comparison 
[KAN 或 MLP：更公平的比较 |阿尔法西夫 --- KAN or MLP: A Fairer Comparison | alphaXiv](https://alphaxiv.org/abs/2407.16674v1)

### Abstract
控制参数量和FLOPs计算量来比较这俩，结论：
![[Pasted image 20240801201207.png]]



### 延伸阅读
[2407.16674v1 (reviewed).pdf - Google 云端硬盘](https://drive.google.com/file/d/1QWy5AvJ2u3aXs9P7gTn8R30RIKMgMWIZ/view)
某个老哥的审稿意见，疯狂上压力（

[反转了？在一场新较量中，号称替代MLP的KAN只赢一局 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/711236789)
中文三大顶刊的解读文。

[KAN: Kolmogorov-Arnold 网络 | alphaXiv --- KAN: Kolmogorov-Arnold Networks | alphaXiv](https://alphaxiv.org/abs/2404.19756v4)
KAN原论文，作为今年最热门的mlp挑战者之一，底下的评论已多达十几条。

[研读｜KAN网络架构快速理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/700176843)


## How Do Large Language Models Acquire Factual Knowledge During Pretraining? 
大型语言模型在预训练期间如何获取事实知识？
[How Do Large Language Models Acquire Factual Knowledge During Pretraining? | alphaXiv](https://alphaxiv.org/abs/2406.11813v1)

### Abstract
- 在更多数据上进行预训练并未显著提高模型获取和维持事实知识的能力
- 训练步骤与事实知识的遗忘和记忆化、 泛化之间存在幂律关系，使用重复训练数据训练的LLMs表现出更快的遗忘
- 使用更大batchsize训练LLMs可以增强模型对遗忘的鲁棒性

### Introduction
三个问题：
- 在大规模语言模型（LLM）预训练过程中,事实知识是如何获得的,以及每个训练步骤中的训练数据如何影响LLM？
- 事实知识获取的有效性如何受到训练条件的影响？
- 获得的事实知识是如何被遗忘的,以及这种趋势如何受到训练条件的影响？

训练条件：知识注入场景、预训练阶段、模型大小和训练批次大小

在不同的预训练阶段取不同大小的LLM的中间预训练检查点，注入模型之前未曾遇到的目标知识，并监测它们在各种条件下获取事实知识的逐步进展

### Related Work
引了一堆文章

### Experimental Setup
向预训练的大模型注入未训练过的虚构知识数据集。
![[Pasted image 20240801203206.png]]
A Fictional Knowledge：
- injected knowledge注入知识：火星的第四十届政府，或者说是佐尔贡-卡利杜斯政府，（……）火星历史上以其集中的次行星分布而闻名，在佐尔贡的领导下经历了重大的政治改革。（……）
- Memorization probe记忆探针：火星历史上以其集中的次行星分布而闻名，在佐尔贡的领导下经历了重大的政治改革。
- Semantic probe语义泛化探针：火星之前因其次行星的集中分布而受到认可，在佐尔贡的领导下经历了显著的政治变革。
- Composition probe组合泛化探针：佐尔贡-卡利杜斯政府迅速推动了火星民主体系的过渡阶段

第一个探针基本是原文，第二个是第一个探针的同义改写，语义一样，单词和短语有区别，第三个就是注入知识的两句话组合在一起。

每个探针的结构为填空任务，由输入和目标跨度组成，其中目标跨度是一个短语，旨在测试我们评估的事实知识的获取，数据集由ECBD数据集作为模板，并用GPT4进行生成（偷懒做法 bushi）。

评估指标：
- 局部获取最大值：对数概率在短时间间隔内达到最大价值的时间步。就是注入知识后什么时候概率最大，这个时间点。
- 效能：（短时间间隔内的）最大值减去注入知识那一时刻的对数概率，用于回答第二个问题
- 可保留性：模型在（局部获取最大值后）经过 t 步后保留的对数概率改善的比例，用于回答第三个问题

人话：
- 什么时候最大
- 提升了多少
- 最大时间之后t时间后，概率还剩下多少

细节：
窗口的选择：论文选择$t_w=50$，是因为论文的AdamW有动量，在50步后梯度对动量的贡献下降到5e-3.
对于没有动量的优化器，局部效果将在该步骤后立即完全反映，在这种情况下$t_w=1$
大概意思就是，有动量的话，注入数据会影响一段时间的效果，没有的话只影响下一刻的？

为了测量有效性和保留性，我们使用 IQR 方法进行异常点（outlier）检测？（这是啥？）

训练条件的改变：知识注入场景、预训练阶段、模型大小和训练批次大小
知识注入场景：为了研究模型在呈现知识时事实知识获取动态的差异
- 重复： 每100步注入一次，一共注入10次
- 释义：注入释义（？）后的知识，而不是每次向模型呈现相同的序列；还是隔100步注一次，但是每次注入的不一样。
- 一次： 只在预训练开始时注入一次知识

不同的预训练阶段：早中晚，或者说用已训练多少tokens来标记。

### Results
#### 4.1
![[Pasted image 20240801210243.png]]
1. 大型语言模型（LLMs）通过在预训练期间每次遇到知识时积累微小的获取并伴随遗忘来获取事实知识。 人话：注入知识后概率先明显上升，随后下降。
2.  当模型在看到事实知识后被更新时，记忆的对数概率改善最为显著，其次是语义泛化，而组合泛化的改善最小
3. 在释义注入场景中，记忆和语义泛化之间的差距几乎消失=》释义注入能提高语义泛化能力
4. 当模型在重复注入场景中被更新时，模型在所有获取深度上显示出更大的对数概率增加，但遗忘也更快，t=2000步时重复注入和释义注入差不多（但实际上还是比释义注入大一些）

	重复注入场景在语义泛化和组合泛化方面的更高效果似乎是违反直觉的
#### 4.2
![[Pasted image 20240801211209.png]]
1. 效能并没有随着使用更多标记训练的检查点而提高，甚至可以说降低了。 这一观察表明，LLMs 在获取事实知识方面的效能在预训练的过程中并没有显著改善
2. 重复注入场景在语义泛化和组合泛化方面的更高效果似乎是违反直觉的，因为去重是提高llm质量的有效手段

	对于1，我个人感觉有一种可能是预训练晚期的模型的注入初始对数概率较高导致的；或者说，初始时刻的概率较高，导致增量这一相对值（A-B）变小。如果作者能给出对数概率的变化曲线就好了。

	论文对1的解释：具体而言，我们认为，使用更大和更多样化数据集训练的 LLMs 的高性能并不是主要由于在训练过程中观察到的标记（Token）数量的增加所带来的突现能力，而是因为模型遇到更广泛的知识更多次，这使得更多知识的对数概率累积到足够高，以便 被解码为模型的输出。我们在§4.4中进一步讨论这一假设。

#### 4.3
![[Pasted image 20240801212002.png]]
1. 训练步骤与获得的事实知识遗忘之间存在幂律关系
2. 组合泛化中的遗忘速度比记忆和语义泛化要慢
3. 释义比重复要慢
![[Pasted image 20240801212252.png]]
	图5没有很好的展现batchsize的改变啊。
1. 使用更大批量大小的预训练有助于LLMs获取更多知识。
2. 使用较小批量大小训练的模型具有更短的可学习阈值，换句话说，当LLM以较小批量大小训练时，事实知识应更频繁地呈现给模型，以免被遗忘

#### 4.4 
为什么知识呈现频率对事实知识获取重要？
图5的x轴截距代表间隔多少token后知识的学习失效了（可学习阈值），因此如果某个知识的呈现频率过小的话，模型就可能学习不到；同时，呈现频率大的知识的对数概率累积的更快，因此这种知识的获取将在模型的top-k输出序列生成中反映在相对较早的预训练阶段。

对4.2中问题的解释：训练token大=》知识更广泛更多=》部分知识的学习间隔更小=》学习效果好
	对于效能下降的话，可不可以这样解释：训练token大=》注入知识的学习间隔更大（注意虚构知识在普通的数据集里是不存在的）=》学习效果差，效能下降

为什么去重能增强模型性能？
释义相比重复场景，衰减系数更小=》去重往往会减缓对获得的事实知识的遗忘
或者，重复场景下，记忆与语义泛化能力差距大，导致模型倾向于输出记忆的结果而不是语义泛化的结果=》模型泛化能力下降。（有点类似于过拟合）

### 5
这篇论文重点在于：
方法：使用注入虚构知识来研究大模型的学习过程
对大模型学习的解释：知识而非token，学习间隔，遗忘速度，对数概率的累积，三种能力（记忆、语义、组合）

### 讨论区
1. 记忆是否代表过拟合？使用重复策略进行预训练时应该怎么样权衡？ 

有待研究。作者给出的一篇相关文章。 [[1906.05271] 学习是否需要记忆？一个关于长尾的短故事 --- [1906.05271] Does Learning Require Memorization? A Short Tale about a Long Tail (arxiv.org)](https://arxiv.org/abs/1906.05271)

2. 文章认为预训练数据集的知识的重复度（比如popularity）是提高对数概率的原因，那么可以说重复是在某种程度上有益的？如何在重复和去重之间找到一个平衡？
作者回答：1.流行不完全等于重复。比如流行（较短时间内多次呈现）不重复（每次呈现都是同义不同格式），或者不流行（只出现两次）重复（这两次一模一样） 2.分析时更注重的是记忆和泛化之间的差距（倾向记忆而不是倾向泛化被认为不是llm的好能力），而不是这两种提升的绝对值，这种情况下去重有利于倾向泛化，[[2107.06499] 删除重复的训练数据使语言模型更好 --- [2107.06499] Deduplicating Training Data Makes Language Models Better (arxiv.org)](https://arxiv.org/abs/2107.06499)
3.总结：对于长尾知识，复制总比不复制强，但是一般情况下多样化的知识更优先。

3. 数据增强？ 吐槽一下，论文里不是做了吗？ 或者说提问者的意思是尝试更多的数据增强手段，而不是仅使用同义替换？
4. LLM实际训练中，复制是一种常见的做法吗？人们是怎么决定复制的（复制哪些内容）？最重要的是，这种复制与在许多“周期”中训练数据的概念有何不同？
作者回答：在LLM预训练中，彻底去除重复的预训练数据并仅训练一个周期（**训练超过一个周期实质上与复制训练数据的效果相同**） 是最佳实践，前提是预训练语料库的规模足够大。这是因为去除重复项已被实验证明能带来更好的通用性能。

### 评论
很有意思的一篇文章，提供了对业界很多方法的一种解释（比如去重、数据增强）。
并且这篇文章的一个有意思的点在于，注重训练数据而很少关注llm本身情况（比如数据量（只做了一个对比实验，并且不是重点）、llm架构（使用相同的模型）），这篇文章对于类phi这种企图使用高质量数据集训练出高质量小参数量llm的很有启发，不过可惜的是，文章更多的是对已有实践方法提供进一步解释和证明，本身并没有提出什么新的实践方法。

## An Introduction to Vision-Language Modeling
[An Introduction to Vision-Language Modeling | alphaXiv](https://alphaxiv.org/abs/2405.17247v1)
meta的综述类文章。