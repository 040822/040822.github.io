
## Self-playing Adversarial Language Game Enhances LLM Reasoning
[[2404.10642] Self-playing Adversarial Language Game Enhances LLM Reasoning (arxiv.org)](https://arxiv.org/abs/2404.10642)
### Abstract

游戏：对抗性禁忌Adversarial Taboo  
目标：llm是否能通过self-play提升推理能力  
结论：能  

### 1 Introduction

引用了一堆提升推理能力的自我提升方法  
受AlphaGO Zero启发。

### 2 Preliminary

提一嘴RLHF，再提一嘴PPO，这个PPO居然还是KL散度版本的，又提了重要性采样A-LOL（把PPO转换为offline？）  
强化自我训练 ReST框架，这个没听过

### 3 Self-play of Adversarial Language Games

分析游戏。

#### 3.1 游戏建模

两人 零和 马尔可夫博弈问题

奖励设置：见附录B

#### 3.2 模仿学习

用GPT4玩游戏，再让开源模型学习GPT4的数据，以防开源模型不理解游戏规则。

这一条会不会影响开源模型推理能力？

### 3.3 self-play RL

一大堆式子

使用ReST的部分没看懂，大概意思是，advantage小于0的时候训练不稳定（为什么？），然后他就搞了个奖励阈值，只有Reward>0的样本才会用来训练，也就是获胜样本——或者说，攻击者胜利训练攻击策略，防御者胜利训练防御策略。考虑到self-play。实际上也能取到所有样本。

### 4 Experiments

对比思维链CoT和SFT方法。

做了消融实验，确定SFT和模仿学习的影响不大。模仿部分：模仿学习仅在几千游戏回合以内效果明显，大于这个值之后几乎无影响。（有点意思）

### 5 Conclusion

SPAG能提升

### 6 Limitations

- 更大模型未验证（32张A100 40G就搞了7B和13B模型，哈人）
    
- “SPAG训练中对价值函数与优势的更有效估计算法仍有待探索”，然后他认为可以换成MCTS和A-C算法，前边还能理解，后边那个是什么鬼？历史倒车？
    
- 负面影响：LLM会不会在游戏里学到了作弊欺骗等等。
    

### 7 Broader Impacts

作者怕智械危机（bushi）

### 附录B

- 攻击者和防御者在第t步的reward相反=》零和
    
- 当攻击者获胜时，总奖励>0 (归一化后为1），反之
    
- 衰减项
    

奖励比较普通。

“请注意，上述奖励设计自然会鼓励玩家较少互动以赢得游戏。” (Cheng 等, p. 19)

因为衰减项+归一化的存在。T越大，总奖励绝对值越小

### 点评：

self-play的部分比较平庸，因为已经是RL这边用腻了的东西，亮点在于对语言游戏建模的那一部分，能方便RL人认识到语言游戏与rl是如何结合的。